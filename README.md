# SQLite ETL Pipeline

## ğŸš€ Project Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** that extracts data from a **Google Sheet**, transforms it, and loads it into an **SQLite database**.

## ğŸ“Œ Features
- **Extract** data directly from Google Sheets using `gspread`.
- **Transform** data (standardize phone numbers, fix date formats, etc.).
- **Load** data into an SQLite database.
- **Verify** successful data loading with SQL queries.

## ğŸ›  Installation
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/baskit-take-home.git
cd baskit-take-home
```

### 2ï¸âƒ£ Create a Virtual Environment (Optional but Recommended)
```bash
python -m venv venv
source venv/bin/activate  # For macOS/Linux
venv\Scripts\activate    # For Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up Google Sheets API Credentials
1. Go to [Google Cloud Console](https://console.cloud.google.com/).
2. Create a new project and enable **Google Sheets API** & **Google Drive API**.
3. Generate a **Service Account JSON key**.
4. Share your Google Sheet with the Service Account email.
5. Save the JSON key as `creds.json` in the project directory.

## ğŸš€ Usage
### 1ï¸âƒ£ Run the ETL Pipeline
```bash
python etl.py
```

### 2ï¸âƒ£ Verify Data in SQLite
To open the database and run queries:
```bash
sqlite3 born_date_data.db
```
Then run:
```sql
SELECT * FROM born_date_data LIMIT 10;
```

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ creds.json         # Google API credentials
â”œâ”€â”€ etl.py             # Main ETL script
â”œâ”€â”€ requirements.txt   # Required Python libraries
â”œâ”€â”€ born_date_data.db  # SQLite database (auto-created)
â”œâ”€â”€ README.md          # Documentation
```
## ğŸ“§ Contact
For any questions, reach out to **nathzjoseph14@gmail.com**

---
ğŸš€ **Happy Coding!** ğŸ˜Š

